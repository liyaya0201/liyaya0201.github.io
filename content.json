[{"title":"Flink之State Backends","date":"2020-05-20T16:00:00.000Z","path":"2020/05/21/Flink之State Backends/","categories":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/tags/Flink/"}]},{"title":"Flink之Checkpoint&Savepoint","date":"2020-05-19T16:00:00.000Z","path":"2020/05/20/Flink之Checkpoint&Savepoint/","categories":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/tags/Flink/"}]},{"title":"Flink之State管理","date":"2020-05-05T16:00:00.000Z","path":"2020/05/06/Flink之State管理/","categories":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://liyaya0201.github.io/tags/Flink/"}]},{"title":"Kafka分区分配策略分析","date":"2020-04-24T16:00:00.000Z","path":"2020/04/25/Kafka分区分配策略分析/","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/tags/Kafka/"}]},{"title":"Azkaban配置Plugin实现Spark作业提交","date":"2020-04-08T16:00:00.000Z","path":"2020/04/09/Azkaban配置Plugin实现Spark作业提交/","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://liyaya0201.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://liyaya0201.github.io/tags/Azkaban/"}]},{"title":"PairRDD的ByKey算子底层剖析","date":"2020-04-01T16:00:00.000Z","path":"2020/04/02/PairRDD的ByKey算子底层剖析/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL自定义HBase外部数据源(仅读)","date":"2020-02-17T16:00:00.000Z","path":"2020/02/18/SparkSQL自定义HBase外部数据源(仅读)/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL自定义外部Text数据源","date":"2020-02-11T16:00:00.000Z","path":"2020/02/12/SparkSQL自定义外部Text数据源/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"spark-shell脚本解读","date":"2020-02-09T16:00:00.000Z","path":"2020/02/10/spark-shell脚本解读/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"JDBC外部数据源实现源码debug","date":"2020-02-05T16:00:00.000Z","path":"2020/02/06/JDBC外部数据源实现源码debug/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Yarn页面显示参数解读","date":"2020-01-17T16:00:00.000Z","path":"2020/01/18/Yarn页面显示参数解读/","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/tags/Hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"https://liyaya0201.github.io/tags/Yarn/"}]},{"title":"Azkaban的安装与常规使用","date":"2019-12-17T16:00:00.000Z","path":"2019/12/18/Azkaban的安装与常规使用/","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://liyaya0201.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://liyaya0201.github.io/tags/Azkaban/"}]},{"title":"Kafka常用的参数调优","date":"2019-12-05T16:00:00.000Z","path":"2019/12/06/Kafka常用的参数调优/","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/tags/Kafka/"}]},{"title":"Flume自定义Source&&Sink&&Interceptor","date":"2019-12-01T16:00:00.000Z","path":"2019/12/02/Flume自定义Source&&Sink&&Interceptor/","categories":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/tags/Flume/"}]},{"title":"记录一次生产上因Spark升级导致的程序报错：java.lang.IllegalArgumentException_ Illegal pattern component_ XXX","date":"2019-11-16T16:00:00.000Z","path":"2019/11/17/记录一次生产上因Spark升级导致的程序报错：java.lang.IllegalArgumentException_ Illegal pattern component_ XXX/","categories":[{"name":"生产故障案例","slug":"生产故障案例","permalink":"https://liyaya0201.github.io/categories/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Spark之监控","date":"2019-10-21T16:00:00.000Z","path":"2019/10/22/Spark之监控/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Kafka常用命令","date":"2019-10-15T16:00:00.000Z","path":"2019/10/16/Kafka入门实战/","categories":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://liyaya0201.github.io/tags/Kafka/"}]},{"title":"SparkCore之分组TopN","date":"2019-10-14T16:00:00.000Z","path":"2019/10/15/SparkCore之分组TopN/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkCore之排序","date":"2019-10-08T16:00:00.000Z","path":"2019/10/09/SparkCore之排序/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"spark.yarn.jars & spark.yarn.archive","date":"2019-09-24T16:00:00.000Z","path":"2019/09/25/spark.yarn.jars && spark.yarn.archive/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Flume的Sink选择器","date":"2019-08-15T16:00:00.000Z","path":"2019/08/16/Flume的Sink选择器/","categories":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/tags/Flume/"}]},{"title":"Flume的Channel选择器","date":"2019-08-14T16:00:00.000Z","path":"2019/08/15/Flume的Channel选择器/","categories":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/tags/Flume/"}]},{"title":"reduceByKey & groupByKey的底层源码解析","date":"2019-08-13T16:00:00.000Z","path":"2019/08/14/reduceByKey & groupByKey的底层源码解析/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"长恨歌","date":"2019-08-13T16:00:00.000Z","path":"2019/08/14/长恨歌/","categories":[{"name":"古诗文","slug":"古诗文","permalink":"https://liyaya0201.github.io/categories/%E5%8F%A4%E8%AF%97%E6%96%87/"}],"tags":[{"name":"notes","slug":"notes","permalink":"https://liyaya0201.github.io/tags/notes/"}]},{"title":"Spark性能优化指南","date":"2019-06-11T16:00:00.000Z","path":"2019/06/12/Spark性能优化指南/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkStreaming+Kafka运行以及调优","date":"2019-05-30T16:00:00.000Z","path":"2019/05/31/SparkStreaming+Kafka运行以及调优/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Spark内存分配","date":"2019-05-27T16:00:00.000Z","path":"2019/05/28/Spark内存分配/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Hadoop中常见的压缩格式对比","date":"2019-05-20T16:00:00.000Z","path":"2019/05/21/Hadoop中常见的压缩格式对比/","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/tags/Hadoop/"}]},{"title":"Spark之序列化","date":"2019-05-14T16:00:00.000Z","path":"2019/05/15/Spark之序列化/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Spark各个版本特性","date":"2019-05-05T16:00:00.000Z","path":"2019/05/06/Spark各个版本特性/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Kafka Offset管理","date":"2019-04-24T16:00:00.000Z","path":"2019/04/25/Kafka Offset管理/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkStreaming对接Kafka","date":"2019-04-22T16:00:00.000Z","path":"2019/04/23/SparkStreaming对接Kafka/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkStreaming简介以及使用","date":"2019-04-17T16:00:00.000Z","path":"2019/04/18/SparkStreaming简介以及使用/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Scala隐式转换","date":"2019-04-15T16:00:00.000Z","path":"2019/04/16/scala隐式转换/","categories":[{"name":"Scala","slug":"Scala","permalink":"https://liyaya0201.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://liyaya0201.github.io/tags/Scala/"}]},{"title":"Flume架构简介","date":"2019-04-09T16:00:00.000Z","path":"2019/04/10/Flume架构简介/","categories":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://liyaya0201.github.io/tags/Flume/"}]},{"title":"SparkSQL执行计划解读","date":"2019-04-07T16:00:00.000Z","path":"2019/04/08/SparkSQL执行计划解读/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL自定义UDAF&UDTF函数","date":"2019-04-01T16:00:00.000Z","path":"2019/04/02/SparkSQL自定义UDAF&UDTF函数/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL的底层容错","date":"2019-03-27T16:00:00.000Z","path":"2019/03/28/SparkSQL的底层容错/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL之DataFrame&DataSet","date":"2019-03-24T16:00:00.000Z","path":"2019/03/25/SparkSQL之DataFrame&DataSet/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL自定义UDF函数","date":"2019-03-17T16:00:00.000Z","path":"2019/03/18/SparkSQL自定义UDF函数/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL使用SQL和API方式完成同一需求","date":"2019-02-21T16:00:00.000Z","path":"2019/02/22/SparkSQL使用SQL和API方式完成同一需求/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL读写数据源","date":"2019-01-23T16:00:00.000Z","path":"2019/01/24/SparkSQL读写数据源/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkSQL初识","date":"2019-01-21T16:00:00.000Z","path":"2019/01/22/SparkSQL初识/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"SparkCore多目录输出&累加器&持久化&广播变量","date":"2019-01-17T16:00:00.000Z","path":"2019/01/18/SparkCore多目录输出&累加器&持久化&广播变量/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"RDD中的窄依赖&宽依赖","date":"2019-01-15T16:00:00.000Z","path":"2019/01/16/RDD中的窄依赖&宽依赖/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Spark术语&Spark提交Yarn的两种模式","date":"2019-01-13T16:00:00.000Z","path":"2019/01/14/Spark术语&Spark提交Yarn的两种模式/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"探究Spark中一个WC程序产生多少个RDD","date":"2019-01-09T16:00:00.000Z","path":"2019/01/10/探究Spark中一个WC程序产生多少个RDD/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"hadoop中数据倾斜解决方案","date":"2018-12-27T16:00:00.000Z","path":"2018/12/28/hadoop中数据倾斜解决方案/","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://liyaya0201.github.io/tags/MapReduce/"}]},{"title":"Spark之RDD算子","date":"2018-12-05T16:00:00.000Z","path":"2018/12/06/Spark之RDD算子/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"RDD五大特性&Spark简单操作","date":"2018-12-04T16:00:00.000Z","path":"2018/12/05/RDD五大特性&Spark简单操作/","categories":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://liyaya0201.github.io/tags/Spark/"}]},{"title":"Yarn三种调度策略","date":"2018-12-02T16:00:00.000Z","path":"2018/12/03/Yarn三种调度策略/","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://liyaya0201.github.io/tags/Hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"https://liyaya0201.github.io/tags/Yarn/"}]}]